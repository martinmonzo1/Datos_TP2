{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import kerastuner as kt\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "import tensorflow_hub as hub\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Importación de los documentos\n",
    "\n",
    "tw_train = pd.read_csv('./train.csv')\n",
    "tw_test = pd.read_csv('./test.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_colwidth = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Preprocesamiento y Feature Extraction a partir de 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "regexes = {'hashtags':r'(#)+', 'mentions':r'(@)\\w*', 'URLs':r'(http://)[a-zA-Z0-9./]*'}\n",
    "\n",
    "referenceDF = pd.read_csv('./headlines_reference_values.csv').drop(columns = 'Unnamed: 0')\n",
    "ref = referenceDF.loc[0]\n",
    "\n",
    "basicStopwords = stopwords.words('english')\n",
    "myStopwords = set(basicStopwords + list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInText(DF, feature):\n",
    "    DF[feature[0]+'_in_t'] = 0   # Inicialización de la nueva columna\n",
    "    for i in range(len(DF)):\n",
    "        try:\n",
    "            if (DF.loc[i, feature]).lower() in (DF.loc[i, 'text']).lower(): DF.loc[i, feature[0]+'_in_t'] = 1\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "def replace_contractions(text, mapping):\n",
    "    noContText = []\n",
    "    counter = 0\n",
    "    for t in text.split(\" \"):\n",
    "        if t.lower() in mapping:\n",
    "            noContText.append(mapping[t.lower()])\n",
    "            counter += 1\n",
    "        else:\n",
    "            noContText.append(t) \n",
    "    return [' '.join(noContText), counter]\n",
    "\n",
    "def Qy(regex, text):   \n",
    "    try:\n",
    "        return len(re.compile(regex).findall(text))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def newFeatures(DF, regexDict):\n",
    "    for k, v in regexDict.items():\n",
    "        DF[['text', k]] = pd.DataFrame(DF['text'].map(lambda x: [re.compile(v).sub(r' ', x), Qy(v, x)]).tolist())\n",
    "\n",
    "def wilsonScore(num, den):\n",
    "    if num == 0:\n",
    "        return round(0, 2)\n",
    "    phat = num/den\n",
    "    z = 1.96\n",
    "    return round(((phat + z*z/(2*den) - z * np.sqrt((phat*(1-phat)+z*z/(4*den))/den))/(1+z*z/den)), 2)\n",
    "\n",
    "def tagging(text):\n",
    "    tagsCount = {'ADJ':0, 'ADP':0, 'ADV':0, 'CONJ':0, 'DET':0, 'NOUN':0, 'NUM':0, 'PRT':0, 'PRON':0, 'VERB':0, '.':0, 'X':0}\n",
    "    tags = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        tags += nltk.pos_tag(word_tokenize(sentence), tagset = 'universal')\n",
    "    for word, tag in tags:\n",
    "        tagsCount[tag] += 1    \n",
    "    POS_ratio = [round((12 - list(tagsCount.values()).count(0))/12, 2)]\n",
    "#     mostCommonPOS_ratio = [round(list(tagsCount.values()).count(max(tagsCount.values()))/(12-list(tagsCount.values()).count(0)), 2)]\n",
    "    mostCommonPOS_ratio = [wilsonScore(list(tagsCount.values()).count(max(tagsCount.values())), 12-list(tagsCount.values()).count(0))]\n",
    "    noun_ratio =  [wilsonScore(tagsCount['NOUN'], sum(tagsCount.values()))]\n",
    "    return list(tagsCount.values()) + POS_ratio + mostCommonPOS_ratio + noun_ratio\n",
    "\n",
    "def headlinesDistance(DF, ref):\n",
    "    DF['dist'] = 0\n",
    "    for i in range(len(DF)):\n",
    "        x = DF.loc[i, ['textLenght','ADJ','ADP','ADV','CONJ','DET','NOUN','NUM','PRT','PRON','VERB','.','X','POS_ratio','topPOS_ratio','NOUN/TOT', 'polarity', 'subjectivity']]\n",
    "        DF.loc[i, 'dist'] =  np.dot(x, ref)/(np.linalg.norm(x)*np.linalg.norm(ref))\n",
    "\n",
    "def sentimentAnalysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment\n",
    "\n",
    "def stemmizer(text, stopwords):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([PorterStemmer().stem(word) for word in words if word not in stopwords])\n",
    "\n",
    "def SWRemoval(text, stopwords):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([word for word in words if word not in stopwords])\n",
    "    \n",
    "def preprocessing(DF, contraction_mapping, regexes, ref, myStopwords):\n",
    "#     originalTextLength = tw_train['text'].map(lambda x: len(x))\n",
    "    isInText(DF, 'keyword')\n",
    "    isInText(DF, 'location')\n",
    "    DF[['text', 'conts']] = pd.DataFrame(DF['text'].map(lambda x: replace_contractions(x, contraction_mapping)).tolist())\n",
    "    originalTextLength = DF['text'].map(lambda x: len(x))\n",
    "    newFeatures(DF, regexes) \n",
    "    DF['textLenght'] = DF['text'].map(lambda x: len(x))\n",
    "#     DF['textTrash'] = originalTextLength - DF['textLenght']\n",
    "    DF[['ADJ','ADP','ADV','CONJ','DET','NOUN','NUM','PRT','PRON','VERB','.','X','POS_ratio','topPOS_ratio','NOUN/TOT']] = pd.DataFrame(DF['text'].map(lambda x: tagging(x)).tolist())\n",
    "    DF[['polarity', 'subjectivity']] = pd.DataFrame(DF['text'].map(lambda x: sentimentAnalysis(x)).tolist())\n",
    "    headlinesDistance(DF, ref)\n",
    "    DF['K'] = DF['textLenght']*DF['NOUN/TOT']*DF['polarity']\n",
    "    DF['text'] = DF['text'].map(lambda x: x.lower())\n",
    "    DF['text'] = DF['text'].map(lambda x: re.compile(r'\\d').sub(r' ', x))\n",
    "#     DF['text'] = DF['text'].map(lambda x: stemmizer(x, myStopwords))\n",
    "    DF['text'] = DF['text'].map(lambda x: SWRemoval(x, myStopwords))\n",
    "    DF['textTrash'] = originalTextLength - DF['textLenght']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Preprocesamiento y feature extraction del text\n",
    "\n",
    "preprocessing(tw_train, contraction_mapping, regexes, ref, myStopwords)\n",
    "preprocessing(tw_test, contraction_mapping, regexes, ref, myStopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Vocabulario - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vocabulario consta de 2173 palabras\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "# las Stopwords ya quedaron afuera en el paso anterior\n",
    "\n",
    "textList = tw_train['text'].tolist()\n",
    "text = ' '.join(textList)\n",
    "tokens = word_tokenize(text)\n",
    "tokensFreq = nltk.FreqDist(tokens)\n",
    "tokensFreq = {k:v for k,v in tokensFreq.items() if (v>5) & (v<150)}\n",
    "myVocabulary = list(tokensFreq.keys())\n",
    "print('El vocabulario consta de {} palabras'.format(len(myVocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "\n",
    "embeddingsFilePath = './glove.twitter.27B.100d.txt'\n",
    "embeddingsFile = open(embeddingsFilePath)\n",
    "embeddingsIndex = {}\n",
    "for line in embeddingsFile:\n",
    "    lineList = line.split()\n",
    "    word = lineList[0]\n",
    "    vector = np.array(lineList[1:])\n",
    "    embeddingsIndex[word] = vector\n",
    "embeddingsFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.82% de los tokens tienen embedding\n"
     ]
    }
   ],
   "source": [
    "def embeddingsInVoc(voc):\n",
    "    noEmbeddingTokens = []\n",
    "    for token in voc:\n",
    "        if token.lower() not in embeddingsIndex:\n",
    "            noEmbeddingTokens.append(token)\n",
    "    print('{}% de los tokens tienen embedding'.format( 100 - round((len(noEmbeddingTokens)/len(voc))*100, 2) ))\n",
    "    return noEmbeddingTokens\n",
    "\n",
    "noEmbeddingTokens = embeddingsInVoc(myVocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokensReplacement = {'icemoon':'ice moon','bioterror':'bio terror','bioterrorism':'bio terrorism','microlight':'micro light',\n",
    "                     'w/heavenly':'heavenly','typhoon-devastated':'typhoon devastated',\"'save\":'save','animalrescue':'animal rescue',\n",
    "                     'mediterran':'mediterranean','meat-loving':'meat loving',\"'suicide\":'suicide'}\n",
    "tokensDeletion = set(tokensReplacement.keys()) ^ set(noEmbeddingTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Últimos detalles sobre la limpieza del texto\n",
    "def replace_noEmbeddingTokens(text, replacement_mapping, deletion_list):\n",
    "    newText = []\n",
    "    for word in word_tokenize(text):\n",
    "        if word.lower() in replacement_mapping:\n",
    "            newText.append(replacement_mapping[word.lower()])\n",
    "        elif word.lower() not in deletion_list:\n",
    "            newText.append(word) \n",
    "    return ' '.join(newText)\n",
    "\n",
    "tw_train['text'] = tw_train['text'].map(lambda x: replace_noEmbeddingTokens(x, tokensReplacement, tokensDeletion))\n",
    "tw_test['text'] = tw_test['text'].map(lambda x: replace_noEmbeddingTokens(x, tokensReplacement, tokensDeletion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El nuevo vocabulario consta de 2112 palabras\n"
     ]
    }
   ],
   "source": [
    "# Redefino vocabulario\n",
    "myVocabulary = [w for w in myVocabulary if w not in tokensDeletion]\n",
    "myVocabulary = [tokensReplacement[w] if (w in tokensReplacement) else w for w in myVocabulary]\n",
    "myVocabulary = list(set(myVocabulary))\n",
    "print('El nuevo vocabulario consta de {} palabras'.format(len(myVocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.67% de los tokens tienen embedding\n"
     ]
    }
   ],
   "source": [
    "noEmbeddingTokens = embeddingsInVoc(myVocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tw_train.to_csv('tw_train.csv', index = False)\n",
    "tw_test.to_csv('tw_test.csv', index = False)\n",
    "\n",
    "with open('myVocabulary.pickle', 'wb') as f:\n",
    "    pickle.dump(myVocabulary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de los resultados"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tw_train = pd.read_csv('tw_train.csv')\n",
    "tw_train['text'] = tw_train['text'].astype('str')\n",
    "tw_test = pd.read_csv('tw_test.csv')\n",
    "tw_test['text'] = tw_test['text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('myVocabulary.pickle', 'rb') as f:\n",
    "    myVocabulary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un Vocabulary Index\n",
    "# El índice cero y el índice 1 se reservan para padding y \"out of index\", respectivamente.\n",
    "\n",
    "vocabularyIndex = {}\n",
    "for i in range(2, len(myVocabulary)+2):  # myVocabulary es una lista ya creada anteriormente\n",
    "    vocabularyIndex[myVocabulary[i-2]] = i\n",
    "    \n",
    "# Construcción de la embedding matrix\n",
    "\n",
    "num_tokens = len(vocabularyIndex) + 2\n",
    "embeddings_dim = 100\n",
    "embeddingsMatrix = np.zeros((num_tokens, embeddings_dim))\n",
    "for word, i in vocabularyIndex.items():\n",
    "    embedding_vector = embeddingsIndex.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddingsMatrix[i] = embedding_vector\n",
    "    else:\n",
    "        continue\n",
    "print('La embeddingsMatrix resultante tiene las dimensiones {}'.format(embeddingsMatrix.shape))\n",
    "\n",
    "# Construcción de los vectores de entrada\n",
    "\n",
    "def vocMapping(text):\n",
    "    vector = []\n",
    "    for word in word_tokenize(text):\n",
    "        try:\n",
    "            vector.append(vocabularyIndex[word])\n",
    "        except:\n",
    "            vector.append(1)  # Embeddings desconocidos\n",
    "    vector = vector + [0]*(num_tokens - len(vector))  # Padding\n",
    "    return np.array(vector).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Data Augmentation: Creación de un nuevo set de train basado en ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Data a utilizar: embeddingsIndex, que es el diccionario tal que (k, v) = (word, embedding)\n",
    "# Lo reduzco a una forma que contenga solo las claves que también se encuentran en myVocabulary\n",
    "\n",
    "myEmbeddingsIndex = {k:v.astype(np.float) for k,v in embeddingsIndex.items() if k in myVocabulary}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# El nuevo texto tendrá el 50% de sus palabras reemplazadas\n",
    "# Se reemplazan las palabras de acuerdo al vector más cercano que exista en el vocabulario\n",
    "\n",
    "def text_adulteration(text, stopwords, embDict):\n",
    "    words = word_tokenize(text)\n",
    "    for i in range(len(words)):\n",
    "        if words[i].lower() in stopwords:\n",
    "            continue\n",
    "        elif words[i].lower() in embDict:\n",
    "            minDistance = -1\n",
    "            for k, v in embDict.items():\n",
    "                if k != words[i].lower():\n",
    "                    distance = np.dot(embDict[words[i].lower()], v)/(np.linalg.norm(embDict[words[i].lower()])*np.linalg.norm(v))\n",
    "                    if (1 - distance) < (1 - minDistance):\n",
    "                        minDistance = distance\n",
    "                        newWord = k\n",
    "        else:\n",
    "            choice = random.randint(0, 1)\n",
    "            if choice == 1:\n",
    "                newWord = list(embDict.keys())[random.randint(0, len(embDict)-1)]\n",
    "            else:\n",
    "                newWord = words[i]\n",
    "        words[i] = newWord\n",
    "    choice = random.randint(0, 1)\n",
    "    if choice == 0:\n",
    "        words.append(list(embDict.keys())[random.randint(0, len(embDict)-1)])\n",
    "    elif (choice == 1) and (len(words) > 1):\n",
    "        words.pop()\n",
    "    return ' '.join(words)\n",
    "\n",
    "def noisy_preprocessing(DF, contraction_mapping, regexes, ref, myStopwords, embDict):\n",
    "#     originalTextLength = tw_train['text'].map(lambda x: len(x))\n",
    "    DF[['text', 'conts']] = pd.DataFrame(DF['text'].map(lambda x: replace_contractions(x, contraction_mapping)).tolist())\n",
    "    originalTextLength = DF['text'].map(lambda x: len(x))\n",
    "    newFeatures(DF, regexes)\n",
    "    DF['text'] = DF['text'].map(lambda x: text_adulteration(x, myStopwords, embDict))\n",
    "    isInText(DF, 'keyword')\n",
    "    isInText(DF, 'location')\n",
    "    DF['textLenght'] = DF['text'].map(lambda x: len(x))\n",
    "#     DF['textTrash'] = originalTextLength - DF['textLenght']\n",
    "    DF[['ADJ','ADP','ADV','CONJ','DET','NOUN','NUM','PRT','PRON','VERB','.','X','POS_ratio','topPOS_ratio','NOUN/TOT']] = pd.DataFrame(DF['text'].map(lambda x: tagging(x)).tolist())\n",
    "    DF[['polarity', 'subjectivity']] = pd.DataFrame(DF['text'].map(lambda x: sentimentAnalysis(x)).tolist())\n",
    "    headlinesDistance(DF, ref)\n",
    "    DF['K'] = DF['textLenght']*DF['NOUN/TOT']*DF['polarity']\n",
    "    DF['text'] = DF['text'].map(lambda x: x.lower())\n",
    "    DF['text'] = DF['text'].map(lambda x: re.compile(r'\\d').sub(r' ', x))\n",
    "#     DF['text'] = DF['text'].map(lambda x: stemmizer(x, myStopwords))\n",
    "    DF['text'] = DF['text'].map(lambda x: SWRemoval(x, myStopwords))\n",
    "    DF['textTrash'] = originalTextLength - DF['textLenght']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def text_mutation(text, embDict):\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i].lower() in embDict:\n",
    "            minDistance = -1\n",
    "            newWord = words[i]\n",
    "            for k, v in embDict.items():\n",
    "                if k != words[i].lower():\n",
    "                    distance = np.dot(embDict[words[i].lower()], v)/(np.linalg.norm(embDict[words[i].lower()])*np.linalg.norm(v))\n",
    "                    if (1 - distance) < (1 - minDistance):\n",
    "                        minDistance = distance\n",
    "                        newWord = k\n",
    "            words[i] = newWord\n",
    "    words.insert(random.randint(0, len(words)-1) ,list(embDict.keys())[random.randint(0, len(embDict)-1)])\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def noisy_preprocessing(DF, contraction_mapping, regexes, ref, myStopwords, myEmbeddingsIndex):\n",
    "    isInText(DF, 'keyword')\n",
    "    isInText(DF, 'location')\n",
    "    DF[['text', 'conts']] = pd.DataFrame(DF['text'].map(lambda x: replace_contractions(x, contraction_mapping)).tolist())\n",
    "    newFeatures(DF, {'hashtags':r'(#)+'})\n",
    "    DF['text'] = DF['text'].map(lambda x: text_mutation(x, myEmbeddingsIndex))\n",
    "    originalTextLength = DF['text'].map(lambda x: len(x))\n",
    "    newFeatures(DF, {'mentions':r'(@)\\w*', 'URLs':r'(http://)[a-zA-Z0-9./]*'})\n",
    "    DF['textLenght'] = DF['text'].map(lambda x: len(x))\n",
    "    DF[['ADJ','ADP','ADV','CONJ','DET','NOUN','NUM','PRT','PRON','VERB','.','X','POS_ratio','topPOS_ratio','NOUN/TOT']] = pd.DataFrame(DF['text'].map(lambda x: tagging(x)).tolist())\n",
    "    DF[['polarity', 'subjectivity']] = pd.DataFrame(DF['text'].map(lambda x: sentimentAnalysis(x)).tolist())\n",
    "    headlinesDistance(DF, ref)\n",
    "    DF['K'] = DF['textLenght']*DF['NOUN/TOT']*DF['polarity']\n",
    "    DF['text'] = DF['text'].map(lambda x: x.lower())\n",
    "    DF['text'] = DF['text'].map(lambda x: re.compile(r'\\d').sub(r' ', x))\n",
    "    DF['text'] = DF['text'].map(lambda x: SWRemoval(x, myStopwords))\n",
    "    DF['textTrash'] = originalTextLength - DF['textLenght']\n",
    "    DF['text'] = DF['text'].map(lambda x: replace_noEmbeddingTokens(x, tokensReplacement, tokensDeletion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tw_train_noise = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "noisy_preprocessing(tw_train_noise, contraction_mapping, regexes, ref, myStopwords, myEmbeddingsIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para ilustrar la diferencia:\n",
      "Original: christian attacked muslims temple mount waving israeli flag via pamela geller ...\n",
      "Mutado: jesus murdered terrorists village inch holding palestinian navy via isis pamela abandoned ...\n"
     ]
    }
   ],
   "source": [
    "print(f'''Para ilustrar la diferencia:\n",
    "Original: {tw_train.loc[500,'text']}\n",
    "Mutado: {tw_train_noise.loc[500,'text']}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tw_train_noise = tw_train_noise[list(tw_train.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tw_train_noise.to_csv('tw_train_noise.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso: Para balancear el Train Set original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(clean, noise):\n",
    "    delta = clean['target'].value_counts().loc[0] - clean['target'].value_counts().loc[1]\n",
    "    if delta > 0:\n",
    "        add_balance = noise.loc[noise['target'] == 1, :].sample(n = abs(delta), replace = False, random_state = 60)\n",
    "    elif delta < 0:\n",
    "        add_balance = noise.loc[noise['target'] == 0, :].sample(n = abs(delta), replace = False, random_state = 60)\n",
    "    balanced = pd.concat([clean, add_balance], axis=0, join = 'outer', ignore_index = True)    \n",
    "    return shuffle(balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso: Para agragar ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(clean, noise, noise_level,keep_proportion = True, return_residual_noise = False):\n",
    "    \n",
    "    if keep_proportion == True:\n",
    "        stratify = noise['target']\n",
    "    else:\n",
    "        stratify = None\n",
    "        \n",
    "    residual_noise, noise_required = train_test_split(noise, stratify = stratify, test_size = noise_level)\n",
    "    noisyDF = pd.concat([clean, noise_required], axis=0, join = 'outer', ignore_index = True)\n",
    "    \n",
    "    if return_residual_noise == False:\n",
    "        return shuffle(noisyDF)\n",
    "    else:\n",
    "        return shuffle(noisyDF), shuffle(residual_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Attention models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET DE DATOS: CON UN SET DE DATOS EXTENDIDO Y BALANCEADO\n",
    "\n",
    "tw_train = pd.read_csv('./tw_train.csv')\n",
    "tw_train['text'] = tw_train['text'].astype('str')\n",
    "tw_train = tw_train.drop(columns = ['l_in_t', 'k_in_t', 'X'])\n",
    "tw_train_noise = pd.read_csv('./tw_train_noise.csv')\n",
    "tw_train_noise['text'] = tw_train_noise['text'].astype('str')\n",
    "tw_train_noise = tw_train_noise.drop(columns = ['l_in_t', 'k_in_t', 'X'])\n",
    "tw_train_noisy, residual_noise = add_noise(tw_train, tw_train_noise, 0.50, keep_proportion = True, return_residual_noise = True)\n",
    "tw_train_noisy_balanced = balance(tw_train_noisy, residual_noise)\n",
    "tw_test = pd.read_csv('./tw_test.csv')\n",
    "tw_test['text'] = tw_test['text'].astype('str')\n",
    "tw_test = tw_test.drop(columns = ['l_in_t', 'k_in_t', 'X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación del set de validación\n",
    "\n",
    "def load_data_holdout(TrainDF, TestDF, validation_fraction, encode_keyword = False):\n",
    "    # Numeric Features: Encoding de 'keyword' + Resultado de feature engineering\n",
    "    print('Preparando las Numeric Features...')\n",
    "    TrainDF_subset = TrainDF.drop(columns = ['id', 'keyword', 'location', 'text', 'target'])\n",
    "    TestDF_subset = TestDF.drop(columns = ['id', 'keyword', 'location', 'text'])\n",
    "    if encode_keyword == True:\n",
    "        ohe = OneHotEncoder(handle_unknown = 'ignore')\n",
    "        ohe.fit(np.array(TrainDF.loc[TrainDF['keyword'].isna() == False, 'keyword']).reshape((-1,1)))\n",
    "        TrainDF['keyword'].fillna('NoKeyword', inplace = True)\n",
    "        TrainDF_keyword_enc = ohe.transform(np.array(TrainDF['keyword']).reshape((-1,1)))\n",
    "        TestDF['keyword'].fillna('NoKeyword', inplace = True)\n",
    "        TestDF_keyword_enc = ohe.transform(np.array(TestDF['keyword']).reshape((-1,1)))\n",
    "        print('El encoding genera {} nuevos features adicionales'.format(TrainDF_keyword_enc.shape[1])) \n",
    "        TrainDF_nf = sparse.hstack((TrainDF_keyword_enc, sparse.csr_matrix(TrainDF_subset.values))).toarray()\n",
    "        X_test_nf = sparse.hstack((TestDF_keyword_enc, sparse.csr_matrix(TestDF_subset.values))).toarray()\n",
    "    else:\n",
    "        TrainDF_nf = TrainDF_subset.to_numpy()\n",
    "        X_test_nf = TestDF_subset.to_numpy()\n",
    "    X_train_nf, X_val_nf, y_train, y_val = train_test_split(TrainDF_nf, TrainDF['target'], stratify=TrainDF['target'], test_size=validation_fraction, random_state=1)\n",
    "    nf_norm_layer = layers.experimental.preprocessing.Normalization()\n",
    "    nf_norm_layer.adapt(X_train_nf)\n",
    "    X_train_nf = nf_norm_layer(X_train_nf)\n",
    "    X_val_nf = nf_norm_layer(X_val_nf)\n",
    "    X_test_nf = nf_norm_layer(X_test_nf)\n",
    "    \n",
    "    # Formateo del texto\n",
    "    print('Preparando el texto... \\n')\n",
    "    X_train_text, X_val_text, y_train_text, y_val_text = train_test_split(TrainDF['text'], TrainDF['target'], stratify=TrainDF['target'], test_size=validation_fraction, random_state=1)\n",
    "    X_test_text = TestDF['text']\n",
    "    \n",
    "    # Formateo del target\n",
    "    y_train = np.array(y_train)\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    print('''Las dimensiones de los sets de datos son:\n",
    "        Set de entrenamiento, features numéricos: {}\n",
    "        Set de entrenamiento, texto: {}\n",
    "        Set de validación, features numéricos: {}\n",
    "        Set de validación, texto: {}\n",
    "        Set de test, features numéricos: {}\n",
    "        Set de test, texto: {}'''.format(X_train_nf.shape, X_train_text.shape, X_val_nf.shape, X_val_text.shape, X_test_nf.shape, X_test_text.shape))\n",
    "\n",
    "    return X_train_text, X_train_nf, X_val_text, X_val_nf, X_test_text, X_test_nf, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando las Numeric Features...\n",
      "Preparando el texto... \n",
      "\n",
      "Las dimensiones de los sets de datos son:\n",
      "        Set de entrenamiento, features numéricos: (10420, 24)\n",
      "        Set de entrenamiento, texto: (10420,)\n",
      "        Set de validación, features numéricos: (2606, 24)\n",
      "        Set de validación, texto: (2606,)\n",
      "        Set de test, features numéricos: (3263, 24)\n",
      "        Set de test, texto: (3263,)\n"
     ]
    }
   ],
   "source": [
    "X_train_text, X_train_nf, X_val_text, X_val_nf, X_test_text, X_test_nf, y_train, y_val = load_data_holdout(tw_train_noisy_balanced, \n",
    "                                                                                                           tw_test, \n",
    "                                                                                                           0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARÁMETROS\n",
    "\n",
    "max_seq_length = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZADOR: Se usa el vocab_file original\n",
    "\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer  # Inicializo tokenizador de BERT\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificación del parámetro max_seq_length...\n",
      "El máximo largo de tokens es 47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Verificación del parámetro max_seq_length...\n",
    "El máximo largo de tokens es {}\n",
    "'''.format(tw_train_noisy_balanced['text'].map(lambda x: [\"[CLS]\"] + tokenizer.tokenize(x) + [\"[SEP]\"]).map(lambda x: len(x)).max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARACION DE LOS INPUTS QUE REQUIERE BERT\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    # Tokenización a partir de la caja negra de BERT...\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return np.array(input_ids)\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    # 1 donde haya token, 0 donde no\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return np.array([1]*len(tokens) + [0] * (max_seq_length - len(tokens)))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    # Segments == 0 para la primer secuencia y == 1 para la segunda\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return np.array(segments + [0] * (max_seq_length - len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULO DE LOS INPUTS\n",
    "\n",
    "trainTokens = X_train_text.map(lambda x: [\"[CLS]\"] + tokenizer.tokenize(x) + [\"[SEP]\"])\n",
    "\n",
    "input_ids = np.stack(trainTokens.map(lambda x: get_ids(x, tokenizer, max_seq_length)))\n",
    "input_masks = np.stack(trainTokens.map(lambda x: get_masks(x, max_seq_length)))\n",
    "input_segments = np.stack(trainTokens.map(lambda x: get_segments(x, max_seq_length)))\n",
    "\n",
    "# trainInputs = [input_ids, input_masks, input_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULO DE LOS INPUTS DE VALIDACION\n",
    "\n",
    "valTokens = X_val_text.map(lambda x: [\"[CLS]\"] + tokenizer.tokenize(x) + [\"[SEP]\"])\n",
    "\n",
    "input_ids_val = np.stack(valTokens.map(lambda x: get_ids(x, tokenizer, max_seq_length)))\n",
    "input_masks_val = np.stack(valTokens.map(lambda x: get_masks(x, max_seq_length)))\n",
    "input_segments_val = np.stack(valTokens.map(lambda x: get_segments(x, max_seq_length)))\n",
    "\n",
    "# trainInputs = [input_ids_val, input_masks_val, input_segments_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = bert_layer\n",
    "#         self.bert = hub.KerasLayer(\n",
    "#             self.bert_path, trainable=self.trainable\n",
    "#         )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-db824f5184e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Primer capa: BERT Custom Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbert_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_fine_tune_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Capas densas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datos/TP2/venvTP2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datos/TP2/venvTP2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-d2859fe0610b>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_trainable_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datos/TP2/venvTP2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m   \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Entradas del modelo\n",
    "in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "# Primer capa: BERT Custom Layer\n",
    "bert_output = BertLayer(n_fine_tune_layers=10)(bert_inputs)\n",
    "\n",
    "# Capas densas \n",
    "# dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "# El modelo\n",
    "BERT_model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "BERT_model.fit([input_ids, input_masks, input_segments], \n",
    "          y_train,\n",
    "          validation_data=([input_ids_val, input_masks_val, input_segments_val], y_val),\n",
    "          epochs=1,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET DE DATOS: CON UN SET DE DATOS EXTENDIDO Y BALANCEADO\n",
    "\n",
    "tw_train = pd.read_csv('./tw_train.csv')\n",
    "tw_train['text'] = tw_train['text'].astype('str')\n",
    "tw_train = tw_train.drop(columns = ['l_in_t', 'k_in_t', 'X'])\n",
    "tw_train_noise = pd.read_csv('./tw_train_noise.csv')\n",
    "tw_train_noise['text'] = tw_train_noise['text'].astype('str')\n",
    "tw_train_noise = tw_train_noise.drop(columns = ['l_in_t', 'k_in_t', 'X'])\n",
    "tw_train_noisy, residual_noise = add_noise(tw_train, tw_train_noise, 0.50, keep_proportion = True, return_residual_noise = True)\n",
    "tw_train_noisy_balanced = balance(tw_train_noisy, residual_noise)\n",
    "tw_test = pd.read_csv('./tw_test.csv')\n",
    "tw_test['text'] = tw_test['text'].astype('str')\n",
    "tw_test = tw_test.drop(columns = ['l_in_t', 'k_in_t', 'X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación del set de validación\n",
    "\n",
    "def load_data_holdout(TrainDF, TestDF, validation_fraction, encode_keyword = False):\n",
    "    # Numeric Features: Encoding de 'keyword' + Resultado de feature engineering\n",
    "    print('Preparando las Numeric Features...')\n",
    "    TrainDF_subset = TrainDF.drop(columns = ['id', 'keyword', 'location', 'text', 'target'])\n",
    "    TestDF_subset = TestDF.drop(columns = ['id', 'keyword', 'location', 'text'])\n",
    "    if encode_keyword == True:\n",
    "        ohe = OneHotEncoder(handle_unknown = 'ignore')\n",
    "        ohe.fit(np.array(TrainDF.loc[TrainDF['keyword'].isna() == False, 'keyword']).reshape((-1,1)))\n",
    "        TrainDF['keyword'].fillna('NoKeyword', inplace = True)\n",
    "        TrainDF_keyword_enc = ohe.transform(np.array(TrainDF['keyword']).reshape((-1,1)))\n",
    "        TestDF['keyword'].fillna('NoKeyword', inplace = True)\n",
    "        TestDF_keyword_enc = ohe.transform(np.array(TestDF['keyword']).reshape((-1,1)))\n",
    "        print('El encoding genera {} nuevos features adicionales'.format(TrainDF_keyword_enc.shape[1])) \n",
    "        TrainDF_nf = sparse.hstack((TrainDF_keyword_enc, sparse.csr_matrix(TrainDF_subset.values))).toarray()\n",
    "        X_test_nf = sparse.hstack((TestDF_keyword_enc, sparse.csr_matrix(TestDF_subset.values))).toarray()\n",
    "    else:\n",
    "        TrainDF_nf = TrainDF_subset.to_numpy()\n",
    "        X_test_nf = TestDF_subset.to_numpy()\n",
    "    X_train_nf, X_val_nf, y_train, y_val = train_test_split(TrainDF_nf, TrainDF['target'], stratify=TrainDF['target'], test_size=validation_fraction, random_state=1)\n",
    "    nf_norm_layer = layers.experimental.preprocessing.Normalization()\n",
    "    nf_norm_layer.adapt(X_train_nf)\n",
    "    X_train_nf = nf_norm_layer(X_train_nf)\n",
    "    X_val_nf = nf_norm_layer(X_val_nf)\n",
    "    X_test_nf = nf_norm_layer(X_test_nf)\n",
    "    \n",
    "    # Formateo del texto\n",
    "    print('Preparando el texto... \\n')\n",
    "    X_train_text, X_val_text, y_train_text, y_val_text = train_test_split(TrainDF['text'], TrainDF['target'], stratify=TrainDF['target'], test_size=validation_fraction, random_state=1)\n",
    "    X_test_text = TestDF['text']\n",
    "    \n",
    "    # Formateo del target\n",
    "    y_train = np.array(y_train)\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    print('''Las dimensiones de los sets de datos son:\n",
    "        Set de entrenamiento, features numéricos: {}\n",
    "        Set de entrenamiento, texto: {}\n",
    "        Set de validación, features numéricos: {}\n",
    "        Set de validación, texto: {}\n",
    "        Set de test, features numéricos: {}\n",
    "        Set de test, texto: {}'''.format(X_train_nf.shape, X_train_text.shape, X_val_nf.shape, X_val_text.shape, X_test_nf.shape, X_test_text.shape))\n",
    "\n",
    "    return X_train_text, X_train_nf, X_val_text, X_val_nf, X_test_text, X_test_nf, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando las Numeric Features...\n",
      "Preparando el texto... \n",
      "\n",
      "Las dimensiones de los sets de datos son:\n",
      "        Set de entrenamiento, features numéricos: (10420, 24)\n",
      "        Set de entrenamiento, texto: (10420,)\n",
      "        Set de validación, features numéricos: (2606, 24)\n",
      "        Set de validación, texto: (2606,)\n",
      "        Set de test, features numéricos: (3263, 24)\n",
      "        Set de test, texto: (3263,)\n"
     ]
    }
   ],
   "source": [
    "X_train_text, X_train_nf, X_val_text, X_val_nf, X_test_text, X_test_nf, y_train, y_val = load_data_holdout(tw_train_noisy_balanced, \n",
    "                                                                                                           tw_test, \n",
    "                                                                                                           0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARÁMETROS\n",
    "\n",
    "max_seq_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZADOR: Se usa el vocab_file original\n",
    "\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer  # Inicializo tokenizador de BERT\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificación del parámetro max_seq_length...\n",
      "El máximo largo de tokens es 47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Verificación del parámetro max_seq_length...\n",
    "El máximo largo de tokens es {}\n",
    "'''.format(tw_train_noisy_balanced['text'].map(lambda x: [\"[CLS]\"] + tokenizer.tokenize(x) + [\"[SEP]\"]).map(lambda x: len(x)).max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARACION DE LOS INPUTS QUE REQUIERE BERT\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    # Tokenización a partir de la caja negra de BERT...\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return np.array(input_ids)\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    # 1 donde haya token, 0 donde no\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return np.array([1]*len(tokens) + [0] * (max_seq_length - len(tokens)))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    # Segments == 0 para la primer secuencia y == 1 para la segunda\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return np.array(segments + [0] * (max_seq_length - len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULO DE LOS INPUTS\n",
    "\n",
    "trainTokens = X_train_text.map(lambda x: [\"[CLS]\"] + tokenizer.tokenize(x) + [\"[SEP]\"])\n",
    "\n",
    "input_ids = np.stack(trainTokens.map(lambda x: get_ids(x, tokenizer, max_seq_length)))\n",
    "input_masks = np.stack(trainTokens.map(lambda x: get_masks(x, max_seq_length)))\n",
    "input_segments = np.stack(trainTokens.map(lambda x: get_segments(x, max_seq_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULO DE LOS INPUTS DE VALIDACION\n",
    "\n",
    "valTokens = X_val_text.map(lambda x: [\"[CLS]\"] + tokenizer.tokenize(x) + [\"[SEP]\"])\n",
    "\n",
    "input_ids_val = np.stack(valTokens.map(lambda x: get_ids(x, tokenizer, max_seq_length)))\n",
    "input_masks_val = np.stack(valTokens.map(lambda x: get_masks(x, max_seq_length)))\n",
    "input_segments_val = np.stack(valTokens.map(lambda x: get_segments(x, max_seq_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTRUCTURA DEL MODELO\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "bert_layer.trainable = False\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "nf_inputs = keras.Input(batch_size = None, shape = (X_train_nf.shape[1],))  # Entrada de los features numéricos completamente pre-procesados\n",
    "nf_outputs = nf_inputs\n",
    "\n",
    "x = layers.Concatenate(axis = 1)([pooled_output, nf_outputs])  # Combinación de las salidas de ambas ramas\n",
    "x = layers.BatchNormalization(axis=-1, trainable=False)(x)\n",
    "\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "BERT_model = keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_9 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 64)           49216       keras_layer_9[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            65          dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 109,531,522\n",
      "Trainable params: 49,281\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BERT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "326/326 - 1471s - loss: 0.6119 - accuracy: 0.6718 - val_loss: 0.5598 - val_accuracy: 0.7134\n",
      "Epoch 2/5\n",
      "326/326 - 1480s - loss: 0.5544 - accuracy: 0.7199 - val_loss: 0.5917 - val_accuracy: 0.6807\n",
      "Epoch 3/5\n",
      "326/326 - 1478s - loss: 0.5471 - accuracy: 0.7255 - val_loss: 0.5038 - val_accuracy: 0.7682\n",
      "Epoch 4/5\n",
      "326/326 - 1484s - loss: 0.5294 - accuracy: 0.7376 - val_loss: 0.5295 - val_accuracy: 0.7444\n",
      "Epoch 5/5\n",
      "326/326 - 1482s - loss: 0.5362 - accuracy: 0.7308 - val_loss: 0.4952 - val_accuracy: 0.7690\n"
     ]
    }
   ],
   "source": [
    "BERT_model_history = BERT_model.fit([input_ids, input_masks, input_segments], y_train, \n",
    "                                    validation_data = ([input_ids_val, input_masks_val, input_segments_val], y_val), \n",
    "                                    batch_size = 32, epochs = 5, verbose = 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTP2",
   "language": "python",
   "name": "venvtp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
